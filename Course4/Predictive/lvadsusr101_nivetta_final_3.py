# -*- coding: utf-8 -*-
"""LVADSUSR101_Nivetta_Final_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SKs-MGLIs0Ug_oFL4MAF6WK7s79Gul6c
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import warnings as wr
wr.filterwarnings("ignore")

# data read
data1 = pd.read_csv('/content/seeds.csv')

print("Missing values")
missing_values = data1.isnull().sum()
missing_values
data1.fillna(method='ffill', inplace=True)

data1.info()

# duplicate check
dup_no = data1.duplicated().sum()
print('Total number of duplicated records: ', dup_no)
# drop duplicates if exist
data1 = data1.drop_duplicates()

# outliers check
q1 = data1.quantile(0.25)
q3 = data1.quantile(0.75)
IQR = q3 - q1
threshold = 1.5
outliers = (data1 < (q1 - threshold * IQR)) | (data1 > (q3 + threshold * IQR))
data = data1[~outliers.any(axis=1)]
print('Number of outliers removed: ', len(data1) - len(data))

# basic eda
# Plot histograms for each feature
for i in data.columns:
    plt.figure()
    sns.histplot(data[i], kde=True)
    plt.title(f'Histogram of {i}')
    plt.xlabel(i)
    plt.ylabel('Frequency')
    plt.show()

# Heatmap for Correlation Matrix
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt = '.2f')
plt.title('Heatmap of Correlation Matrix')
plt.show()

plt.figure(figsize=(15, 10))
for i, col in enumerate(data.columns):
    plt.subplot(2, 4, i + 1)
    sns.histplot(data[col], kde=True, color = 'skyblue')
    plt.title(col)
plt.tight_layout()
plt.show()

scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(scaled_data)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

#KMeans model
clf = KMeans(n_clusters=3,random_state=42)
clf.fit(scaled_data)
data['Cluster'] = clf.labels_

#Model evaluation
silhouette_avg = silhouette_score(scaled_data, kmeans.labels_)
print("Silhouette Score:", silhouette_avg)
sns.pairplot(data=data, hue='Cluster', palette = 'viridis')
plt.show()