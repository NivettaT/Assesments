# -*- coding: utf-8 -*-
"""LVADSUSR101_Nivetta_Final_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VV1AdGn8nUfeXUFxwAzbnqOuC-JamUap
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import warnings as wr
wr.filterwarnings("ignore")

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
# data read
data1 = pd.read_csv('/content/loan_approval.csv')
data1.info()
data1.describe()
data1.head(5)

# duplicate check
dup_no = data1.duplicated().sum()
print('Total number of duplicated records: ', dup_no)
# drop duplicates if exist
data1 = data1.drop_duplicates()

# null check
data1.isnull().sum()
# if null is there, then impute with mean if normally distributed else with median

# outliers check
q1 = data1.quantile(0.25)
q3 = data1.quantile(0.75)
IQR = q3 - q1
threshold = 1.5
outliers = (data1 < (q1 - threshold * IQR)) | (data1 > (q3 + threshold * IQR))
data = data1[~outliers.any(axis=1)]
print('Number of outliers removed: ', len(data1) - len(data))

data.head()

# drop unecessary clms
data = data.drop('loan_id',axis = 1)

# Plot histograms for each feature
for i in data.columns:
    plt.figure()
    sns.histplot(data[i], kde=True)
    plt.title(f'Histogram of {i}')
    plt.xlabel(i)
    plt.ylabel('Frequency')
    plt.show()

# countplot for Target Variable
sns.countplot(data[' loan_status'])

# Heatmap for Correlation Matrix
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt = '.2f')
plt.title('Heatmap of Correlation Matrix')
plt.show()

# Encode categorical data using Label Encoding
label_encoder = LabelEncoder()
for column in data.columns:
    if data[column].dtype == 'object':
        data[column] = label_encoder.fit_transform(data[column])

data.head()

# extract feature and labels
X = data.drop([' loan_status'], axis=1)
y = data[' loan_status']

# train test splits
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# model build using Logistic Reg
model = LogisticRegression()
model.fit(X_train, y_train)
pred_lr = model.predict(X_test)

# model build using Logistic Reg
DT = DecisionTreeClassifier()
DT.fit(X_train, y_train)
pred_dt = DT.predict(X_test)

# evaluation metric for log reg
accuracy = accuracy_score(y_test, pred_lr)
precision = precision_score(y_test, pred_lr)
recall = recall_score(y_test, pred_lr)
f1 = f1_score(y_test, pred_lr)
conf_matrix = confusion_matrix(y_test, pred_lr)

# evaluation metric for log reg
accuracy_dt = accuracy_score(y_test, pred_dt)
precision_dt = precision_score(y_test, pred_dt)
recall_dt = recall_score(y_test, pred_dt)
f1_dt = f1_score(y_test, pred_dt)
conf_matrix_dt = confusion_matrix(y_test, pred_dt)

print("Accuracy using logistic reg:", accuracy)
print("Precision using logistic reg:", precision)
print("Recall using logistic reg:", recall)
print("F1 Score using logistic reg:", f1)
print("Confusion Matrix using logistic reg:\n", conf_matrix)

print("\n")

print("Accuracy using decision tree:", accuracy_dt)
print("Precision using decision tree:", precision_dt)
print("Recall using decision tree:", recall_dt)
print("F1 Score using decision tree:", f1_dt)
print("Confusion Matrix using decision tree:\n", conf_matrix_dt)