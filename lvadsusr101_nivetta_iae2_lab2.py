# -*- coding: utf-8 -*-
"""LVADSUSR101_Nivetta_IAE2_Lab2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-v7_fpYD1YDwQS9N4aFdPWvdMTLtLQeL
"""

#Import
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import warnings as wr
wr.filterwarnings("ignore")

# data read
data1 = pd.read_csv("/content/Mall_Customers.csv")
data1.head()

# basic data1 explr
print("The total number of rows are "+ str(data1.shape[0]))
print("The total number of columns are "+ str(data1.shape[1]))
data1.info()
data1.describe()

# find duplicates
data1.duplicated().sum()
data_no_dup = data1.drop_duplicates()
print("The total number of duplicates dropped are: ",len(data1)-len(data_no_dup))

# null values
data_no_dup.isnull().sum()
data_no_dup['Annual Income (k$)'] = data_no_dup['Annual Income (k$)'].fillna(data_no_dup['Annual Income (k$)'].mean())

# duplicates
q1 = data_no_dup.quantile(0.25)
q3 = data_no_dup.quantile(0.75)
IQR = q3 - q1
threshold = 1.5
outliers = (data_no_dup < (q1 - threshold * IQR)) | (data_no_dup > (q3 + threshold * IQR))
data = data_no_dup[~outliers.any(axis=1)]
print('Number of outliers removed: ', len(data_no_dup) - len(data))

# drop unnecessary columns
data_cleaned = data.drop(['CustomerID'], axis=1)
data_cleaned.head()

# basic eda
for i in data.columns:
    plt.figure()
    sns.histplot(data[i], kde=True)
    plt.title(f'Histogram of {i}')
    plt.xlabel(i)
    plt.ylabel('Frequency')
    plt.show()

# correl between attributes
plt.figure(figsize=(8, 6))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

#Normalization
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']])

# feature engineering
data['Spending to Income Ratio'] = data['Spending Score (1-100)'] / data['Annual Income (k$)']

#Elbow method
Elbow_List = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(data_scaled)
    Elbow_List.append(kmeans.inertia_)
plt.figure(figsize=(10, 5))
plt.plot(range(1, 11), Elbow_List, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Elbow_List')
plt.show()
# insights: we cam see that the line gradually falls to a standstill at around 8 clusters, so that will the the optimum value used

# Silhouette Score
silhouette_scores_list = []
for n_clusters in range(2, 11):
    kmeans = KMeans(n_clusters=n_clusters)
    cluster_labels = kmeans.fit_predict(data_scaled)
    silhouette_avg = silhouette_score(data_scaled, cluster_labels)
    silhouette_scores_list.append(silhouette_avg)
plt.figure(figsize=(10, 5))
plt.plot(range(2, 11), silhouette_scores_list, marker='o')
plt.title('Silhouette Score')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.show()

n_clusters = 8

# k-means clustering algo
kmeans = KMeans(n_clusters=n_clusters)
data['Cluster'] = kmeans.fit_predict(data_scaled)
cluster_means = data.groupby('Cluster').mean()
cluster_counts = data['Cluster'].value_counts()

#Visualization
pca = PCA(n_components=2)
data_pca = pca.fit_transform(data_scaled)
data['PC1'] = data_pca[:, 0]
data['PC2'] = data_pca[:, 1]

plt.figure(figsize=(10, 6))
sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=data,palette = 'viridis', alpha=0.8)

plt.title('Clusters Visualization (PCA)')
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.show()